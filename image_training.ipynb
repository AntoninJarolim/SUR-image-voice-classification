{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.datasets\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.transforms.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Eye(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(2)\n",
    "        #40x40\n",
    "        self.conv2a = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv2b = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(32)\n",
    "        #20x20\n",
    "        self.conv3a = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.conv3b = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(64)\n",
    "        #10x10\n",
    "\n",
    "        self.conv4a = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv4b = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(64)\n",
    "        # 5x5\n",
    "\n",
    "        self.lin1 = nn.Linear(5*5*64, 64)\n",
    "        self.lin2 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        #40x40\n",
    "        x = self.conv2a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.pool1(x)\n",
    "        #20x20\n",
    "        x = self.conv3a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.pool1(x)\n",
    "        #10x10\n",
    "        x = self.conv4a(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4b(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        x = self.pool1(x)\n",
    "        #5x5x128\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return torch.clamp(tensor + torch.randn(tensor.size(), dtype=torch.float) * self.std + self.mean, 0, 1)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_torch = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(),\n",
    "    v2.RandomAffine(15, [0.05,0.05], [0.9,1.1]),\n",
    "    v2.GaussianBlur(3),\n",
    "    v2.RandomApply([AddGaussianNoise(0, 0.15)], p=0.3),\n",
    "    v2.RandomPosterize(bits=4),\n",
    "    v2.RandomSolarize(threshold=0.9),\n",
    "    v2.RandomAutocontrast(),\n",
    "    v2.RandomAdjustSharpness(sharpness_factor=2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = torchvision.datasets.ImageFolder(\"training\", transform=to_torch)\n",
    "target_valid = torchvision.datasets.ImageFolder(\"validation\", transform=to_torch)\n",
    "\n",
    "target_count = len([x[1] for x in target_train if x[1] == 0])\n",
    "class_weights = [1/target_count, 1/(len(target_train)-target_count)]\n",
    "class_weights = [class_weights[x[1]] for x in target_train]\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(class_weights, len(target_train), replacement=True)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(target_train, batch_size=30, sampler=sampler)\n",
    "val_loader = torch.utils.data.DataLoader(target_valid, batch_size=len(target_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(torchvision.transforms.functional.to_pil_image(transforms(x[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Eye()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "lfunc = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 85\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for batch in iter(loader):\n",
    "        inputs, labels = batch\n",
    "        inputs = transforms(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        result = model(inputs)\n",
    "        loss:torch.Tensor = lfunc(result, labels.reshape([len(labels),1]).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch\n",
    "            result = model(inputs)\n",
    "            loss:torch.Tensor = lfunc(result, labels.reshape([len(labels),1]).float())\n",
    "            print(i, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs = transforms(inputs)\n",
    "        result = model(inputs)\n",
    "        print((result < 0.5) == (labels.reshape([len(labels),1]) < 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"visual_model.pth\", \"wb\") as f:\n",
    "    torch.save(model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
